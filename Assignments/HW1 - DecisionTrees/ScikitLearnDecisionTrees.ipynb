{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1 : Decision Tress using `scikit-learn`\n",
    "\n",
    "Scikit-learn provides a range of supervised and unsupervised learning algorithms via a consistent interface in Python. In this assigment you'll explore how to train decision trees using the `scikit-learn` library. The scikit-learn documentation can be found [here](http://scikit-learn.org/stable/documentation.html).\n",
    "\n",
    "In this assignment we'll attempt to classify patients as either having or not having diabetic retinopathy. For this task we'll be using the Diabetic Retinopathy data set, which contains 1151 instances and 20 attributes (some categorical, some continuous). You can find additional details about the dataset [here](http://archive.ics.uci.edu/ml/datasets/Diabetic+Retinopathy+Debrecen+Data+Set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You may add additional imports\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1151, 20)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>quality</th>\n",
       "      <th>prescreen</th>\n",
       "      <th>ma2</th>\n",
       "      <th>ma3</th>\n",
       "      <th>ma4</th>\n",
       "      <th>ma5</th>\n",
       "      <th>ma6</th>\n",
       "      <th>ma7</th>\n",
       "      <th>exudate8</th>\n",
       "      <th>exudate9</th>\n",
       "      <th>exudate10</th>\n",
       "      <th>exudate11</th>\n",
       "      <th>exudate12</th>\n",
       "      <th>exudate13</th>\n",
       "      <th>exudate14</th>\n",
       "      <th>exudate15</th>\n",
       "      <th>euDist</th>\n",
       "      <th>diameter</th>\n",
       "      <th>amfm_class</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>19</td>\n",
       "      <td>18</td>\n",
       "      <td>14</td>\n",
       "      <td>49.895756</td>\n",
       "      <td>17.775994</td>\n",
       "      <td>5.270920</td>\n",
       "      <td>0.771761</td>\n",
       "      <td>0.018632</td>\n",
       "      <td>0.006864</td>\n",
       "      <td>0.003923</td>\n",
       "      <td>0.003923</td>\n",
       "      <td>0.486903</td>\n",
       "      <td>0.100025</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>22</td>\n",
       "      <td>18</td>\n",
       "      <td>16</td>\n",
       "      <td>13</td>\n",
       "      <td>57.709936</td>\n",
       "      <td>23.799994</td>\n",
       "      <td>3.325423</td>\n",
       "      <td>0.234185</td>\n",
       "      <td>0.003903</td>\n",
       "      <td>0.003903</td>\n",
       "      <td>0.003903</td>\n",
       "      <td>0.003903</td>\n",
       "      <td>0.520908</td>\n",
       "      <td>0.144414</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>62</td>\n",
       "      <td>60</td>\n",
       "      <td>59</td>\n",
       "      <td>54</td>\n",
       "      <td>47</td>\n",
       "      <td>33</td>\n",
       "      <td>55.831441</td>\n",
       "      <td>27.993933</td>\n",
       "      <td>12.687485</td>\n",
       "      <td>4.852282</td>\n",
       "      <td>1.393889</td>\n",
       "      <td>0.373252</td>\n",
       "      <td>0.041817</td>\n",
       "      <td>0.007744</td>\n",
       "      <td>0.530904</td>\n",
       "      <td>0.128548</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>55</td>\n",
       "      <td>53</td>\n",
       "      <td>53</td>\n",
       "      <td>50</td>\n",
       "      <td>43</td>\n",
       "      <td>31</td>\n",
       "      <td>40.467228</td>\n",
       "      <td>18.445954</td>\n",
       "      <td>9.118901</td>\n",
       "      <td>3.079428</td>\n",
       "      <td>0.840261</td>\n",
       "      <td>0.272434</td>\n",
       "      <td>0.007653</td>\n",
       "      <td>0.001531</td>\n",
       "      <td>0.483284</td>\n",
       "      <td>0.114790</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>44</td>\n",
       "      <td>44</td>\n",
       "      <td>44</td>\n",
       "      <td>41</td>\n",
       "      <td>39</td>\n",
       "      <td>27</td>\n",
       "      <td>18.026254</td>\n",
       "      <td>8.570709</td>\n",
       "      <td>0.410381</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.475935</td>\n",
       "      <td>0.123572</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>44</td>\n",
       "      <td>43</td>\n",
       "      <td>41</td>\n",
       "      <td>41</td>\n",
       "      <td>37</td>\n",
       "      <td>29</td>\n",
       "      <td>28.356400</td>\n",
       "      <td>6.935636</td>\n",
       "      <td>2.305771</td>\n",
       "      <td>0.323724</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.502831</td>\n",
       "      <td>0.126741</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>29</td>\n",
       "      <td>29</td>\n",
       "      <td>27</td>\n",
       "      <td>25</td>\n",
       "      <td>16</td>\n",
       "      <td>15.448398</td>\n",
       "      <td>9.113819</td>\n",
       "      <td>1.633493</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.541743</td>\n",
       "      <td>0.139575</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>20.679649</td>\n",
       "      <td>9.497786</td>\n",
       "      <td>1.223660</td>\n",
       "      <td>0.150382</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.576318</td>\n",
       "      <td>0.071071</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>21</td>\n",
       "      <td>18</td>\n",
       "      <td>15</td>\n",
       "      <td>13</td>\n",
       "      <td>10</td>\n",
       "      <td>66.691933</td>\n",
       "      <td>23.545543</td>\n",
       "      <td>6.151117</td>\n",
       "      <td>0.496372</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500073</td>\n",
       "      <td>0.116793</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>79</td>\n",
       "      <td>75</td>\n",
       "      <td>73</td>\n",
       "      <td>71</td>\n",
       "      <td>64</td>\n",
       "      <td>47</td>\n",
       "      <td>22.141784</td>\n",
       "      <td>10.054384</td>\n",
       "      <td>0.874633</td>\n",
       "      <td>0.099780</td>\n",
       "      <td>0.023386</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.560959</td>\n",
       "      <td>0.109134</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   quality  prescreen  ma2  ma3  ma4  ma5  ma6  ma7   exudate8   exudate9  \\\n",
       "0        1          1   22   22   22   19   18   14  49.895756  17.775994   \n",
       "1        1          1   24   24   22   18   16   13  57.709936  23.799994   \n",
       "2        1          1   62   60   59   54   47   33  55.831441  27.993933   \n",
       "3        1          1   55   53   53   50   43   31  40.467228  18.445954   \n",
       "4        1          1   44   44   44   41   39   27  18.026254   8.570709   \n",
       "5        1          1   44   43   41   41   37   29  28.356400   6.935636   \n",
       "6        1          0   29   29   29   27   25   16  15.448398   9.113819   \n",
       "7        1          1    6    6    6    6    2    1  20.679649   9.497786   \n",
       "8        1          1   22   21   18   15   13   10  66.691933  23.545543   \n",
       "9        1          1   79   75   73   71   64   47  22.141784  10.054384   \n",
       "\n",
       "   exudate10  exudate11  exudate12  exudate13  exudate14  exudate15    euDist  \\\n",
       "0   5.270920   0.771761   0.018632   0.006864   0.003923   0.003923  0.486903   \n",
       "1   3.325423   0.234185   0.003903   0.003903   0.003903   0.003903  0.520908   \n",
       "2  12.687485   4.852282   1.393889   0.373252   0.041817   0.007744  0.530904   \n",
       "3   9.118901   3.079428   0.840261   0.272434   0.007653   0.001531  0.483284   \n",
       "4   0.410381   0.000000   0.000000   0.000000   0.000000   0.000000  0.475935   \n",
       "5   2.305771   0.323724   0.000000   0.000000   0.000000   0.000000  0.502831   \n",
       "6   1.633493   0.000000   0.000000   0.000000   0.000000   0.000000  0.541743   \n",
       "7   1.223660   0.150382   0.000000   0.000000   0.000000   0.000000  0.576318   \n",
       "8   6.151117   0.496372   0.000000   0.000000   0.000000   0.000000  0.500073   \n",
       "9   0.874633   0.099780   0.023386   0.000000   0.000000   0.000000  0.560959   \n",
       "\n",
       "   diameter  amfm_class  label  \n",
       "0  0.100025           1      0  \n",
       "1  0.144414           0      0  \n",
       "2  0.128548           0      1  \n",
       "3  0.114790           0      0  \n",
       "4  0.123572           0      1  \n",
       "5  0.126741           0      1  \n",
       "6  0.139575           0      1  \n",
       "7  0.071071           1      0  \n",
       "8  0.116793           0      1  \n",
       "9  0.109134           0      1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the data from csv file\n",
    "col_names = []\n",
    "for i in range(20):\n",
    "    if i == 0:\n",
    "        col_names.append('quality')\n",
    "    if i == 1:\n",
    "        col_names.append('prescreen')\n",
    "    if i >= 2 and i <= 7:\n",
    "        col_names.append('ma' + str(i))\n",
    "    if i >= 8 and i <= 15:\n",
    "        col_names.append('exudate' + str(i))\n",
    "    if i == 16:\n",
    "        col_names.append('euDist')\n",
    "    if i == 17:\n",
    "        col_names.append('diameter')\n",
    "    if i == 18:\n",
    "        col_names.append('amfm_class')\n",
    "    if i == 19:\n",
    "        col_names.append('label')\n",
    "\n",
    "data = pd.read_csv(\"messidor_features.txt\", names = col_names)\n",
    "print(data.shape)\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data preprocessing  & dimensionality reduction with PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1. Separate the feature columns from the class label column. You should end up with two separate data frames - one that contains all of the feature values and one that contains the class labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    0\n",
      "1    0\n",
      "2    1\n",
      "3    0\n",
      "4    1\n",
      "5    1\n",
      "6    1\n",
      "7    0\n",
      "8    1\n",
      "9    1\n",
      "Name: label, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>quality</th>\n",
       "      <th>prescreen</th>\n",
       "      <th>ma2</th>\n",
       "      <th>ma3</th>\n",
       "      <th>ma4</th>\n",
       "      <th>ma5</th>\n",
       "      <th>ma6</th>\n",
       "      <th>ma7</th>\n",
       "      <th>exudate8</th>\n",
       "      <th>exudate9</th>\n",
       "      <th>exudate10</th>\n",
       "      <th>exudate11</th>\n",
       "      <th>exudate12</th>\n",
       "      <th>exudate13</th>\n",
       "      <th>exudate14</th>\n",
       "      <th>exudate15</th>\n",
       "      <th>euDist</th>\n",
       "      <th>diameter</th>\n",
       "      <th>amfm_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>19</td>\n",
       "      <td>18</td>\n",
       "      <td>14</td>\n",
       "      <td>49.895756</td>\n",
       "      <td>17.775994</td>\n",
       "      <td>5.270920</td>\n",
       "      <td>0.771761</td>\n",
       "      <td>0.018632</td>\n",
       "      <td>0.006864</td>\n",
       "      <td>0.003923</td>\n",
       "      <td>0.003923</td>\n",
       "      <td>0.486903</td>\n",
       "      <td>0.100025</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>22</td>\n",
       "      <td>18</td>\n",
       "      <td>16</td>\n",
       "      <td>13</td>\n",
       "      <td>57.709936</td>\n",
       "      <td>23.799994</td>\n",
       "      <td>3.325423</td>\n",
       "      <td>0.234185</td>\n",
       "      <td>0.003903</td>\n",
       "      <td>0.003903</td>\n",
       "      <td>0.003903</td>\n",
       "      <td>0.003903</td>\n",
       "      <td>0.520908</td>\n",
       "      <td>0.144414</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>62</td>\n",
       "      <td>60</td>\n",
       "      <td>59</td>\n",
       "      <td>54</td>\n",
       "      <td>47</td>\n",
       "      <td>33</td>\n",
       "      <td>55.831441</td>\n",
       "      <td>27.993933</td>\n",
       "      <td>12.687485</td>\n",
       "      <td>4.852282</td>\n",
       "      <td>1.393889</td>\n",
       "      <td>0.373252</td>\n",
       "      <td>0.041817</td>\n",
       "      <td>0.007744</td>\n",
       "      <td>0.530904</td>\n",
       "      <td>0.128548</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>55</td>\n",
       "      <td>53</td>\n",
       "      <td>53</td>\n",
       "      <td>50</td>\n",
       "      <td>43</td>\n",
       "      <td>31</td>\n",
       "      <td>40.467228</td>\n",
       "      <td>18.445954</td>\n",
       "      <td>9.118901</td>\n",
       "      <td>3.079428</td>\n",
       "      <td>0.840261</td>\n",
       "      <td>0.272434</td>\n",
       "      <td>0.007653</td>\n",
       "      <td>0.001531</td>\n",
       "      <td>0.483284</td>\n",
       "      <td>0.114790</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>44</td>\n",
       "      <td>44</td>\n",
       "      <td>44</td>\n",
       "      <td>41</td>\n",
       "      <td>39</td>\n",
       "      <td>27</td>\n",
       "      <td>18.026254</td>\n",
       "      <td>8.570709</td>\n",
       "      <td>0.410381</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.475935</td>\n",
       "      <td>0.123572</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>44</td>\n",
       "      <td>43</td>\n",
       "      <td>41</td>\n",
       "      <td>41</td>\n",
       "      <td>37</td>\n",
       "      <td>29</td>\n",
       "      <td>28.356400</td>\n",
       "      <td>6.935636</td>\n",
       "      <td>2.305771</td>\n",
       "      <td>0.323724</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.502831</td>\n",
       "      <td>0.126741</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>29</td>\n",
       "      <td>29</td>\n",
       "      <td>27</td>\n",
       "      <td>25</td>\n",
       "      <td>16</td>\n",
       "      <td>15.448398</td>\n",
       "      <td>9.113819</td>\n",
       "      <td>1.633493</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.541743</td>\n",
       "      <td>0.139575</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>20.679649</td>\n",
       "      <td>9.497786</td>\n",
       "      <td>1.223660</td>\n",
       "      <td>0.150382</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.576318</td>\n",
       "      <td>0.071071</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>21</td>\n",
       "      <td>18</td>\n",
       "      <td>15</td>\n",
       "      <td>13</td>\n",
       "      <td>10</td>\n",
       "      <td>66.691933</td>\n",
       "      <td>23.545543</td>\n",
       "      <td>6.151117</td>\n",
       "      <td>0.496372</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500073</td>\n",
       "      <td>0.116793</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>79</td>\n",
       "      <td>75</td>\n",
       "      <td>73</td>\n",
       "      <td>71</td>\n",
       "      <td>64</td>\n",
       "      <td>47</td>\n",
       "      <td>22.141784</td>\n",
       "      <td>10.054384</td>\n",
       "      <td>0.874633</td>\n",
       "      <td>0.099780</td>\n",
       "      <td>0.023386</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.560959</td>\n",
       "      <td>0.109134</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   quality  prescreen  ma2  ma3  ma4  ma5  ma6  ma7   exudate8   exudate9  \\\n",
       "0        1          1   22   22   22   19   18   14  49.895756  17.775994   \n",
       "1        1          1   24   24   22   18   16   13  57.709936  23.799994   \n",
       "2        1          1   62   60   59   54   47   33  55.831441  27.993933   \n",
       "3        1          1   55   53   53   50   43   31  40.467228  18.445954   \n",
       "4        1          1   44   44   44   41   39   27  18.026254   8.570709   \n",
       "5        1          1   44   43   41   41   37   29  28.356400   6.935636   \n",
       "6        1          0   29   29   29   27   25   16  15.448398   9.113819   \n",
       "7        1          1    6    6    6    6    2    1  20.679649   9.497786   \n",
       "8        1          1   22   21   18   15   13   10  66.691933  23.545543   \n",
       "9        1          1   79   75   73   71   64   47  22.141784  10.054384   \n",
       "\n",
       "   exudate10  exudate11  exudate12  exudate13  exudate14  exudate15    euDist  \\\n",
       "0   5.270920   0.771761   0.018632   0.006864   0.003923   0.003923  0.486903   \n",
       "1   3.325423   0.234185   0.003903   0.003903   0.003903   0.003903  0.520908   \n",
       "2  12.687485   4.852282   1.393889   0.373252   0.041817   0.007744  0.530904   \n",
       "3   9.118901   3.079428   0.840261   0.272434   0.007653   0.001531  0.483284   \n",
       "4   0.410381   0.000000   0.000000   0.000000   0.000000   0.000000  0.475935   \n",
       "5   2.305771   0.323724   0.000000   0.000000   0.000000   0.000000  0.502831   \n",
       "6   1.633493   0.000000   0.000000   0.000000   0.000000   0.000000  0.541743   \n",
       "7   1.223660   0.150382   0.000000   0.000000   0.000000   0.000000  0.576318   \n",
       "8   6.151117   0.496372   0.000000   0.000000   0.000000   0.000000  0.500073   \n",
       "9   0.874633   0.099780   0.023386   0.000000   0.000000   0.000000  0.560959   \n",
       "\n",
       "   diameter  amfm_class  \n",
       "0  0.100025           1  \n",
       "1  0.144414           0  \n",
       "2  0.128548           0  \n",
       "3  0.114790           0  \n",
       "4  0.123572           0  \n",
       "5  0.126741           0  \n",
       "6  0.139575           0  \n",
       "7  0.071071           1  \n",
       "8  0.116793           0  \n",
       "9  0.109134           0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelcol = data[\"label\"]\n",
    "print(labelcol.head(10))\n",
    "\n",
    "features = data.drop(\"label\", axis=1)\n",
    "features.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2. Use `sklearn.preprocessing.StandardScaler` to standardize the datasetâ€™s features (mean = 0 and variance = 1). \n",
    "\n",
    "Only standardize the the features, not the class labels! This will be required for running the principal component analysis (PCA) dimensionality reduction later. Note that `StandardScaler` returns a numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.05905386  0.2982129  -0.6414863  ... -1.29476283 -0.46865568\n",
      "   1.40504812]\n",
      " [ 0.05905386  0.2982129  -0.56339113 ... -0.08216786  2.00605415\n",
      "  -0.7117194 ]\n",
      " [ 0.05905386  0.2982129   0.92041699 ...  0.27428264  1.1215164\n",
      "  -0.7117194 ]\n",
      " ...\n",
      " [ 0.05905386 -3.35330894  0.41279842 ...  1.33436273  1.19371332\n",
      "  -0.7117194 ]\n",
      " [ 0.05905386  0.2982129   0.0223226  ... -1.32796165 -0.09707846\n",
      "   1.40504812]\n",
      " [ 0.05905386  0.2982129  -1.22720003 ...  1.17603538 -1.08570243\n",
      "  -0.7117194 ]]\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "features = scaler.fit_transform(features)\n",
    "print(features)      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3 . Split your dataset into training and test sets (80% - 20% split). Use `sklearn.model_selection.train_test_split` to help you in this task. You should be working with your standardized features from here forward. Display how many records are in the training set and how many are in the test set.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Records in training set: (920, 19)\n",
      "Records in test set: (231, 19)\n"
     ]
    }
   ],
   "source": [
    "training_feat, test_feat = train_test_split(features, train_size = 0.8, test_size = 0.2)\n",
    "training_label, test_label = train_test_split(labelcol, train_size = 0.8, test_size = 0.2)\n",
    "print(\"Records in training set:\", training_feat.shape)\n",
    "print(\"Records in test set:\", test_feat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4. PCA is affected by the scale of the features that is why it is important to standardize the dataset first. The principle components generated by PCA are sensitive to the shape of the data in d-dimensional space. \n",
    "* Carry out a principal components analysis using `sklearn.decomposition.PCA` Note that you are fitting PCA on the **training set** only (NOT the test set).\n",
    "* Use the `pca.explained_variance_ratio_` field to determine how many principal components are needed so that 95% variance is retained. \n",
    "* Reduce the PCA-transformed-dataset to this number of columns and you'll use the resulting dataset for subsequent tasks.\n",
    "\n",
    "* Once the training set's dimensionality has been reduced with PCA, transform the **test set** to the principal component space that was created. (Do not fit a new PCA. Use the same one that was created with the training set.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance in each PC: [3.18907436e-01 2.80563066e-01 1.08599931e-01 5.89317109e-02\n",
      " 4.91481759e-02 4.64364645e-02 4.04945005e-02 3.59499674e-02\n",
      " 2.95120930e-02 1.20740110e-02 7.25283517e-03 5.98927849e-03\n",
      " 2.52083801e-03 1.30417072e-03 1.12749809e-03 8.01068125e-04\n",
      " 2.09165203e-04 1.13793993e-04 6.39959264e-05]\n",
      "Cumulative variance: [0.31890744 0.5994705  0.70807043 0.76700214 0.81615032 0.86258678\n",
      " 0.90308128 0.93903125 0.96854335 0.98061736 0.98787019 0.99385947\n",
      " 0.99638031 0.99768448 0.99881198 0.99961304 0.99982221 0.999936\n",
      " 1.        ]\n",
      "n_cols to keep: 9\n",
      "[False False False False False False False False  True  True  True  True\n",
      "  True  True  True  True  True  True  True]\n",
      "(920, 9) (231, 9)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA()\n",
    "pca_data = pca.fit_transform(training_feat)\n",
    "\n",
    "var = pca.explained_variance_ratio_\n",
    "print(\"Variance in each PC:\", var)\n",
    "\n",
    "cum_var = np.cumsum(var)\n",
    "print(\"Cumulative variance:\", cum_var)\n",
    "\n",
    "numCol = 1 + np.argmax(cum_var > 0.95)\n",
    "print(\"n_cols to keep:\", numCol)\n",
    "\n",
    "print(cum_var > 0.95)\n",
    "\n",
    "training_pca = pca_data[:, :numCol]\n",
    "test_scale = scaler.transform(test_feat)\n",
    "test_pca = pca.transform(test_scale)[:, :numCol]\n",
    "print(training_pca.shape, test_pca.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Training Decision Trees in `scikit-learn`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5. Use `sklearn.tree.DecisionTreeClassifier` to fit a decision tree classifier on the training set. Use entropy as the split criterion. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Text(181.35000000000002, 190.26, 'X[8] <= 1.015\\nentropy = 0.997\\nsamples = 920\\nvalue = [430, 490]'),\n",
       " Text(111.60000000000001, 135.9, 'X[7] <= -0.186\\nentropy = 0.998\\nsamples = 901\\nvalue = [427, 474]'),\n",
       " Text(55.800000000000004, 81.53999999999999, 'X[1] <= -0.15\\nentropy = 0.999\\nsamples = 421\\nvalue = [218, 203]'),\n",
       " Text(27.900000000000002, 27.180000000000007, 'entropy = 0.989\\nsamples = 299\\nvalue = [168, 131]'),\n",
       " Text(83.7, 27.180000000000007, 'entropy = 0.976\\nsamples = 122\\nvalue = [50, 72]'),\n",
       " Text(167.4, 81.53999999999999, 'X[2] <= 1.083\\nentropy = 0.988\\nsamples = 480\\nvalue = [209, 271]'),\n",
       " Text(139.5, 27.180000000000007, 'entropy = 0.98\\nsamples = 429\\nvalue = [179, 250]'),\n",
       " Text(195.3, 27.180000000000007, 'entropy = 0.977\\nsamples = 51\\nvalue = [30, 21]'),\n",
       " Text(251.10000000000002, 135.9, 'X[5] <= 0.134\\nentropy = 0.629\\nsamples = 19\\nvalue = [3, 16]'),\n",
       " Text(223.20000000000002, 81.53999999999999, 'entropy = 0.0\\nsamples = 11\\nvalue = [0, 11]'),\n",
       " Text(279.0, 81.53999999999999, 'X[3] <= -0.457\\nentropy = 0.954\\nsamples = 8\\nvalue = [3, 5]'),\n",
       " Text(251.10000000000002, 27.180000000000007, 'entropy = 0.0\\nsamples = 4\\nvalue = [0, 4]'),\n",
       " Text(306.90000000000003, 27.180000000000007, 'entropy = 0.811\\nsamples = 4\\nvalue = [3, 1]')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV0AAADnCAYAAAC9roUQAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO2deXhURda438pCEkAMGQiggsCAMjgoCmiUEAdiFIQRkgHChyjMp+JCBBy2mAAiTSBIlCDMCIIKPxxGcSZEZfzi8CkNEsHAIHwG2RkCCEnYAsQsZDm/Pzpps0KW7r7dTb3Pcx9I971V59xT93TdOqeqlIig0Wg0GsfgYbQAGo1GcyOhna5Go9E4EO10NRqNxoFop6vRaDQORDtdjUajcSDa6Wo0Go0D0U5Xo9FoHIh2uhqNRuNAtNPVaDQaB6Kdrkaj0TgQ7XQ1Go3GgWinq9FoNA5EO12NRqNxINrpajQajQPRTlej0WgciHa6Go1G40C009VoNBoHop2uRqPROBDtdDUajcaBaKer0Wg0DsTLaAE0Gj8/v8yCgoI2RsthD3x9fbPy8/PbGi2HxnlQejdgjdEopcRd26FSChFRRsuhcR708IJGo9E4ED28oHFqkpOT8fPzIzs7m65du7Jy5UqmTZvGgQMH2LdvH0eOHGHVqlXMnz+fPn36MHDgwGplFBUV4e3tXWsdy5YtIz09neXLl1s/27hxI/v370cpxdSpU5k3bx65ubnEx8ezevVqjh07Rs+ePYmIiLCL3hr3Rfd0NU7NsGHD2LRpEydPniQoKIjWrVvTrVs3/Pz8OHnyJD4+Pnh6etKvX79K1+Xk5LBmzRri4+Mxm81kZGSQmJhoPbKzs63nRkVF4e/vX+n67du3M23aNC5evIiIMHPmTOt3LVq0QClFXl6efZXXuCXa6WqcmuLiYgoLC7ly5Uqlzw8cOMDSpUvp1KlTJQdazuzZs8nIyOCpp54iLCwMEaG4uNh61HUMuabzIiIieP3119m7d2+dy9FoytHDCxqnJjExkUmTJnHo0CE+++wz6+etWrVizpw5nD9/npYtW1a77u233yY3N5fPPvuMgIAABg4cyNSpU2us46OPPiItLY0tW7Zw8803U1JSwoMPPsiiRYsICAhAKcWKFStIS0tjz549ZGdns2vXLry8vFBKx8g09UNnL2gMpz7ZC2+99RaPP/443bp1q/T5O++8Q1BQEPfee689RGwwOntBUxXtdDWGU9+UsejoaOLj4+0iy5IlSygqKqJz587WINn777/PmTNn8PLy4sUXXyQmJgZvb2+ioqL4z3/+Q3p6OsnJyZjN5mrlaaerqYoeXtA4JZ988glHjx7l8uXLjB8/ntjYWLp06cKQIUPYt28f69at4+rVq2RkZPDII4+QnJyMUoqoqCimT59OeHg4Bw8epFOnTvTu3Ztdu3bRq1cvunfvzsWLF1mzZo21riFDhtClSxcAsrOziYuLIyYmxup09+7dy5IlS4iIiGDAgAEEBQVxxx13sH79el599VV69OjBzz//bMh90rgeOpCmcUq+/PJL2rZti4hQWFjIQw89xLhx49i9ezd33XUXo0ePBiA8PJxTp04RERFBZGQkZrMZf39/IiMjuXDhAiNHjiQpKYl9+/bRvXt3gGpBtdLS0mvKEhkZycKFCyksLKRXr16cPn2aTZs2WdPQ1q5dy5gxY+x7QzRug+7papySsLAwjh8/TseOHa1pYWWv6rRp04ZVq1bh5eVFkyZNCA4OJjEx0drTTUlJYfny5TRr1gwfHx9KS0vp06ePteyAgIBag2qBgYEkJCTQu3dv9uzZQ0lJCSKCp6cnkZGReHhY+ilFRUU8++yzAGRkZHD77bfb/6Zo3AI9pqsxHFtPA6445luen7to0SK8vBzfx9BjupqqaKerMRy99oLmRkIPL2jcmsZkOpSWlvLKK6/g5+fHsGHDuHLlCjt37iQzM5O3334bk8kEwIABA+jbt68txda4Mdrpapya5cuX4+XlRY8ePfDx8WHr1q1kZWURFxdHaGgoISEh5Ofn4+HhwaOPPorZbKZt27YcO3aMOXPmAJUzIZ577jlWrlxJ9+7drcGvjIwMNmzYYK1z9OjRBAYGcuHCBVq3bs348eOZNWsWK1asICwsjKioKC5evEjTpk2ZMmUKMTEx2ulq6ozOXtA4Nffddx/nz5/nypUr5OXl4evry4EDBwC49957mTZtGl5eXkRHR/Pdd98BlvUaevbsSXp6OlA5EyIgIAB/f3/Onj1rraO2KcKtWrUiICCANWvWWGe9LVu2jDFjxujpv5oGo3u6GqfmwoULNGvWjP379+Ph4YG/vz+FhYUAeHl54eHhYf233BGuW7eOI0eOMHToUJKSkiplQuTk5FjLKykpwdPTk44dO9aazeDp6UlBQQFjx45l1apVbN26lYKCAh544AHy8vIwmUwMHjzYYfdD4/roQJrGcGwZSJszZw7R0dH4+vrapLzGogNpmqpop6sxHJ29oLmR0GO6GqcnOjq6UdebzWZiY2MByM3NZdCgQRw4cICdO3eSkJBgLd9kMmEymUhNTa1WRsXrPv/8c+bMmcOrr75a7brDhw8zbty4RsmrcW+009U4BbGxsYgIs2fPJisri6VLl/L8889bFwo3m82kpKRY/42Pj+eNN95gxYoV1jJSU1Oti5T/5S9/qVR++SLnK1euJDw8HMA6Sy03N9eajTBr1iz++c9/VpOv4nVms5lZs2Zx6dIlzpw5U+m6rl270rat3odSUzva6WqcgtDQUL744gu8vb0pLS2ltLQUpRRHjhwBrK/plJSUALBt2zYCAwO5dOmStYzS0tJKWQhVOXnyJJmZmZjNZuuKYFOnTqV9+/bVshGKioqsdVW97rnnnmPhwoWcPn0aT09Pe9wOjRujsxc0TkH//v0JDg5mxYoVHD16lJKSkkqL0dx5550sWLAAgEGDBhEcHExOTo51ERuw9GarbttTkfbt27Nw4UJWr15NUFAQGzZs4PDhw2RnZxMQEFApG2Ht2rX079+fTp06VbuuuLgYHx8fQkNDCQwM1FkMmnqhA2kaw7F3IC09PZ2vvvqKSZMm1fma7OxsAgMD613X4cOHSU5OZtq0aYAOpGmqo52uxnD8/PwyCwoK2hgthz3w9fXNys/P14O8Git6TFdjOPn5+W1FRNV2AGFAJjAd8LjWuY44gGeBc8DI652rHa6mKrqnq3FalFIeQDQQBTwpIpsNFsmKUqoX8HcgCYgWkSKDRdK4CNrpapwSpZQ/sAYIBEaIyCmDRaqGUupXwIdAUyBSRDINFknjAujhBY3ToZS6G9gJnAAedkaHCyAi54EhwGZgl1Iq2GCRNC6A7ulqnAql1BhgMTBZRP5qtDx1RSn1OPABMB94223nNWsajXa6GqdAKdUEeAt4DPiDiPyfwSLVG6VUZ+AfwAHgORHJNVgkjROihxc0hqOUug0wA7cBfVzR4QKIyDHgIaAA+E4pdafBImmcEO10NYailOoPpAGfAxEikmOwSI1CRPKB/waWANuUUuEGi6RxMvTwgsYQlFIKmAa8AjwlIv9rsEg2RynVB0ta2UdArIhUXxBCc8Ohna7G4SilWmAJOt0GDBeRkwaLZDeUUq2AdVjWORklItkGi6QxGD28oHEoSqm7sKSDZQMh7uxwAUTkHDAI+Bb4t1LqQYNF0hiMdroah6GUGoUlYDZfRF4UkUKDRXIIIlIiIjOBl4BPlVITyoZXNDcgenhBY3eUUt7AIuD3WNLB9hgskmEopbpgSSv7P+AFEfnZYJE0Dkb3dDV2RSnVDsuMrS5A7xvZ4QKIyBGgfIhhe5kT1txAaKersRtKqRBgF5ACPCEiFw0WySkQkTzgaWA58K1S6gmDRdI4ED28oLE5ZeOVr2BZinGsiHxpsEhOi1IqCPgE+H/AbBEpMVgkjZ3RTldjU5RSNwHvAZ2xpIMdN1Yi50cpFQj8DSgFRovIWYNF0tgRPbygsRlKqd9gmV2WAwRrh1s3ynJ3H8MyFLNLKXW/wSJp7Ih2uhqboJQaDmwBFonIeBEpMFomV0JEikXkVWASsFEp9YJOK3NP9PCCplEopbyAeOAPWNLBdhssksujlLoDy44Uu4AXy9Zz0LgJuqeraTBKqbbAV8BdQC/tcG2DiBwCHgCaYMlu6GywSBobop2upkEopfpi6YltBgaLyAWDRXIryiZNPAm8D+xQSg02WCSNjdDDC5o6o5TyBQqBiUAM8EcR+cJYqdyfsh+4j7E44NcBbz1m7rpop6upE2XjjP/E0rvthmX89pixUt04lA3lfIRlgXR/YIqIpBorlaYh6OEFTV2JA34FtAYGaYfrWMp2Gh4C5GP50XvDWIk0DUU7Xc11Kdu/bDjgiWV4oamxEt2w+GGZQFEEPKTXbXBN9PCCpk4opVoCOXqXW+dAKdVSr2Xhmminq9FoNA7Ey2gBbiT8/PwyCwoK2hgth73w9fXNys/Pb2u0HO6IO7Qd3T4s6J6uA1FKufXbuVIKEdFTV+2AO7Qd3T4s6ECaRqPROBA9vOAkJCcn4+fnR3Z2Nl27dmXlypVMmzaNy5cv8+2335KUlMQXX3zB4sWL6dOnDwMHDqxWRlFREd7e3tety2QyATBgwAD69u0LwMaNG1m2bBkpKSnk5uYyY8YMWrRoQWhoKM2bN8dsNtO+fXuefPJJ2yquaRC1tZcdO3Zw7NgxevbsSUREBCaTqcHt5Xrt5MqVK7z//vscO3aMiRMn8utf/5r33nuPw4cPEx8fbx/F3QDtdJ2EYcOGMXXqVAICAnjqqadITk6mW7duAPTs2ZOffvqJ5s2b069fPwoKfpmMlJOTw6effsqZM2fo1asXd9xxBxs2bLB+P3r0aAIDA61/X7x4kaZNmzJlyhRiYmKsD9OQIUPYtm0bAJ6enpw7d468vDw6dOjAsmXL6NChA67+eutO1NZefvzxR5RS5OXlATS4vdSlndx0003cd999fPPNN3h7e/P1119z9913c/jwYQfeCddDDy84CcXFxRQWFnLlypVq323YsIGhQ4fWeN3s2bPJyMjgqaeeIiwsDBGhuLjYepQ7ytOnT5OYmEh+/vUXrMrKymLEiBEsWLCA5ORkMjMzmTx5Mrt27dKO10morb1ERETw+uuvs3fv3hptVdf2Ulc79+vXj+joaA4dOkRqaiqpqans2rWL3NzcxivppuierpOQmJjIpEmTOHToEJ999lml77Zt28bSpUtrvO7tt98mNzeXzz77jICAAAYOHMjUqVOrnXfLLbcwefJkAPLy8jCZTAwePJhNmzbRrVs3Tpw4QVpaGh9++CGPPfYYGzduZMeOHQwbNoxu3boxb948fH190Uu8Oge1tZd//etf7Nq1Cy8vrxptVdf2EhAQcN128tBDD7F+/XrOnDnDiy++yKxZswDIzMykefPm9lPexdHZCw6kPhHot956i8cff9w6xFDOO++8Q1BQEPfee689RGwUOjptP67Xdlyhvej2YUE7XQdS37Sf6OhouwUklixZQlFREZ07dyYiIgKA999/nzNnzuDl5cWLL75ITEwM3t7eREVF8dVXX3H58mUyMjJq7XXrh8p+1KXtOLq97Nixwxpgfeyxx1ixYgUnTpzgT3/6E6mpqda2NGPGjHIddPtADy8YzieffMLRo0e5fPky48ePJzY2li5dujBkyBD27dvHunXruHr1KhkZGTzyyCMkJyejlCIqKorp06cTHh7OwYMH6dSpE71792bXrl306tWL7t27c/HiRdasWWOta8iQIXTpYpmun52dTVxcHDExMdaHaO/evSxZsoSIiAgGDBhAUFAQd9xxB+vXr6dt27acOnWKFi1aGHKfNBacqb2sW7fOGmBt1aoVsbGxJCUlcfr06UptSUT0sFQFdCDNYL788kvatm2LiFBYWMhDDz3EuHHj2L17N3fddRejR48GIDw8nFOnThEREUFkZCRmsxl/f38iIyO5cOECI0eOJCkpiX379tG9e3eAakGS0tLSa8oSGRnJwoULKSwspFevXpw+fZpNmzbh7e3N+fPnSUxMpLCwUAfTDMSZ2kvVAOvevXs5evQo/fv3r9SWtMOtjO7pGkxYWBjHjx+nY8eO+Pj44OnpWf4aRps2bVi1ahVeXl40adKE4OBgEhMTrT2XlJQUli9fTrNmzfDx8aG0tJQ+ffpYyw4ICKgxSAIQGBhIQkICvXv3Zs+ePZSUlCAieHp6EhkZiYeH5fe4qKiIZ599lr/+9a+YTCbdazEYZ2ovY8aMsQZYs7OzeeaZZxg1ahQ//PBDpbakqYKI6MNBh+V2244ZM2ZY/3/8+HGZPHmyFBUV2bSO+lCmn+H32R0PW7Qdo9uLbh+WQwfSHIg7zJ+/FjpQYj/coe3o9mFBj+m6CdHR0Q2+trS0lEmTJhEdHc2OHTvYuXMnc+fOZfbs2QAsW7aMF154wVaiapyExrQZqNwudu7cyYwZM3j55ZcpLi62hXhuix7TdRKWL1+Ol5cXPXr0wMfHh61bt5KVlUVcXByhoaGEhISQn5+Ph4cHjz76KGazmbZt23Ls2DHmzJkDVI5sP/fcc6xcuZLu3bszZswYADIyMmqc8nnhwgVat27N+PHjmTVrFq1bt8ZkMrFs2TLOnDlDVFRUox9Qje0xss0AldrFli1bePnll1m3bh07d+7kwQcfdOzNcCF0T9dJuO+++zh//jxXrlwhLy8PX19fDhw4AMC9997LtGnT8PLyIjo6mu+++w6wzL/v2bMn6enpQOXIdkBAAP7+/pw9e9Zah0jNUz5btWpFQEAAa9asoWXLlg7WXNNQjGwzVRkzZgxr1qzhxx9/rNOiSzcyuqfrJFy4cIFmzZqxf/9+PDw88Pf3p7CwEAAvLy88PDys/5Y3/HXr1nHkyBGGDh1KUlJSpch2Tk6OtbySkhI8PT3p2LFjrdFpT09PCgoKGDt2LFeuXMFkMlFcXEy7du346KOPSEtLY8uWLTz88MMOuyeaa2N0m6nYLjp16oSPjw933HEHvXr1ctg9cEV0IM2B2DIYMmfOHKKjo/H19bVJebZAB0rshy3ajtFtRrcPC9rpOhB3iEBfC/1Q2Q93aDu6fVjQY7oajUbjQLTTdTIamyVgNpuJjY0FIDc3l0GDBnHgwAE2bdrE/PnzmThxImBZ4m/mzJm89tpr1cqoeF1aWhqJiYmEhISQm5tb6bvDhw8zbty4RsmrsS22bD9Lly4lOjqa1NTUaudVTBc7f/48c+bMISEhgZKSEtauXcvy5csbJYc7owNpBhAbG8u8efN47bXXmDBhAuvXryc9PZ3FixcDloZfUFCAr68vBQUF7NmzBw8PD26++Waef/55AFJTU9m5cycATZo04aWXXrKW369fPwBWrlxJeHg4YJk+GhYWRlRUFAATJ05k8eLFDBs2rJp8Fa+7//77K+1csXjxYut3Xbt2pW3bG35zV4fjqPYTEhLCwoUL8fPzqyZDxXSxv/3tb3h6elrXaujXrx8pKSn2uwEuju7pGkBoaChffPEF3t7elJaWUlpailKKI0eOANaxL0pKSgDLIuaBgYFcunTJWkZpaWmlVJ6qnDx5kszMTMxmM2azGbD0TsrzLwH+85//0KlTJ4qKiqx11XRd+c4VNX2ncTyOaD8A99xzD0uWLOH777+v1EaqcvXqVfr370/Hjh3ZsWOHjbV1P3RP1wD69+9PcHAwK1as4OjRo5SUlFRa1enOO+9kwYIFAAwaNIjg4GBycnKsq0GBpTdR3iOpifbt27Nw4UJWr15NUFAQq1atYuvWrRQUFPDAAw+wfft2awL72rVr6d+/P506dap2HVTeuaLqdxrH44j2c/HiRd555x0uXbrE4MGDK7URqJwuFhERQWJiIsXFxZhMpkrOXVMdnb3gQBwRgU5PT+err75i0qRJdb4mOzu70uaVdeXw4cMkJyczbdo0QEen7Ymjshdqaz/1aSP/+Mc/aN68OY899lilz3X7sKCdrgPx8/PLLCgoaGO0HPbC19c3Kz8/Xw/y2gF3aDu6fVjQTtdJUUoFAv8LbARiHZWkqZRqB3wFfALMcfnkUA3KsgDyFmCViPy/BlybCvxZRP5qD/luNHQgzQkpc3xmIAkHOlwAETkD/A4IB+YrvWK5OxAGBAL1dpplbW8mMEcppWNANkA7XSdDKXUbFof7oYgY0tMUkWygP/AY8KZ2vK5Lme1MwGsiUnP6wXUQka+BU8DTtpTtRkU7XSdCKXU7ltfAd0VkvpGyiMh5IBQIBpYqpXRbcU2GAH5YhosawyxgllKqSeNFurHRD5KToJT6NRaHmygibxotD4CIXMTyanovsEI7XteizF5zsfRyr73L5HUQkW3AQeC/bSHbjYx+iJwApdQdwGZgvogsNVqeiojIJWAgcAfwvlLK02CRNHUnHCgFkm1U3mxgplLKeZa2c0G00zUYpVR3LA73NRF512h5akJErgCPA7cBa3VAxfkp+3GcC8yyVVxARNKAfwPP26K8GxXtdA1EKXU3lrSwGSLygdHyXAsR+Rn4PdAS+JtSSm8P4NxEApeB/7FxubOBaKVUMxuXe8Ogna5BKKXuA/4FTBaRD42Wpy6ISD4wDPAB/q6U8jFYJE0NlL2JzAFm2jr7RUT2AtuACbYs90ZCT44wAKXU/cDnwPMiYqvxNodRFsFeBzQF/lDmjDVOglLqj8BYoL89Ug7LhsTMQBcRuWzr8t0d7XQdjFKqL7AB+KOI/NNoeRpKWW9qLdAKGCoieQaLpMH6g3gQeFpEvrFjPR8CB0XEZK863BXtdB2IUuphLPmSY0TkX0bL01jKgjUfAB2AISKSa7BINzxKqReAcBF57LonN66ersB2oGtZaqGmjmin6yCUUo9geSUfVTbDxy0oc7wrgG7A4/p10zjKUrkOYxnySXNAfe8Bp0Vklr3rcie003UASqlBwBosD4PdXvmMoiwJfxnQCxioez7GoJSaCDwiIk84qL6OWFLI7hSRc46o0x3QTtfOKKWeAFYBT4iI2y6rXzbHfzEQAoSVTSPWOAilVFPgKDBIRPY4sN6/ALkiMt1Rdbo62unaEaXUcODPwGAR2WW0PPamzPHGA4Ow9LiyDRbphkEpNQ24X0RGOLjeW4EfgO4ikunIul0V7XTthFLqv4C3cHDPw2jKHO/rwHAgtGypSI0dUUrdBBwBBojIPgPqXwx4iEjdtyu5gdFO1w4opcYC84HHRCTdaHmMQCk1E3gKiyP4yWh53Jmye91NRMZc92T71N8G+BHoKSInjZDBldBO18YopZ7FMhvoERE5YLA4hqKUmo5lnv4AEckwWh53RCnljyVj4SEROWygHPGAv4i8YJQMroJ2ujZEKTUBmI7F4Rr2ADgTSqnJwGQsjveY0fK4G0qpucBtImLokotKqV9hmZTRR0T+Y6Qszo52ujZCKfUK8DKWcUzd6CqglHoReBXLj9Eho+VxF5RSrbA4ul4ictxgcVBKvQ60N/oHwNnRTrcRKKX8gAIsvdtnsTjcE8ZK5ZwopZ7BstTgI8B/gEK96WXjUEotBFqIyItGywKVhjreBb4Wka8MFskp0euiNo4tQBoWR/I7HTCqHRF5Tyl1FctOw+ayf98zVCgXRinVFssP/T1Gy1KBcCwBtVFABhYba6qgl3ZsIEqpbsBvgSeAN7XDrROfYlk/+PeAU/TOXJhoYK2InDJakAp8DvwMdAZuNVgWp0X3dBvObCwb/p0AcgyWxVUoAc4C+UAvpVQbEckyWCaXQinVG3gFyxZKdxksTiVE5JxSajCwBMv0YE0N6DHdBqKUugXw0mO49adsAkUv4N96XLd+KKUexzJmegb4UkRmGiySpp7onm4DEZHTRsvgqpQ5WrefFm0nArG8un8POMWu0Zr64VQ9XT8/v8yCgoI2RsthL3x9fbPy8/PbGi2HvXBn+zmL7ZRSdwJPY4eteCriDrZ0FptVxamcrlLKrd82lVKIiDJaDnvhzvZzd9tVxR1s6aw209kLGo1G40D0mK5G0whc8TXcWV+7bxRcbnghOTkZPz8/srOz6dq1KytXrmTatGkUFxczc+ZM4uPj6datG2vXruXnn3/mhReqr79RVFSEt7f3deVZsmQJRUVFdO7cmYiICAC2bdvGzJkzSUlJwdfXl5EjR/LQQw8xcuRIbrnlluvp55SvO7biWvarzW4nT55k586dZGZm8vbbb2MymejTpw8DBw6sVsb17LZs2TLS09NZvny59bONGzeyf/9+lFK88MILzJgxgxYtWhAaGopSit27d1NSUkJ0dPT1dKvRdq74Gl6XdlhRr9psd+XKFbZs2cK5c+eIj49vlO3KMZks+1wOGDCAvn37Wj+fNWsWt956KwMHDiQmJoagoCAmTpxIUlISJ06cYPPmzXz66af11tMIXG54YdiwYWzatImTJ08SFBRE69at6datG7/97W8ZNmyY9bx+/fpVui4vL4/169cTHx/Pp59+ysWLF0lMTLQeR44cqVZXdnY2U6dOZdeuXwLtwcHB/O53v7P+HRgYyLlz5/DwcLlb6VBqs1tYWBgxMTGUlpYC1e2Wk5PDmjVriI+Px2w2k5GRUclu2dm/rJMeFRWFv79/peu3b9/OtGnTuHjxIp6enpw7d47MzEw6dOhASEgIOTk5XL161f43wIWpzXZ9+vQBIDfXsh9pY2wHcPHiRZo2bcqsWbP45z9/2Sh7/fr1DBgwAAAvLy9atmzJlStXKC0tJSIign79+hEeHm7PW2BTXM5TFBcXU1hYyJUrV+p13eLFi0lLS2P48OEMHz4cEaG4uNh6lD/0ubm5JCYmcuBA3VZlXLZsGVOmTOG99/SM1mtxLbstW7aMMWNqXgp29uzZZGRk8NRTTxEWFlbNbnXtZYoImZmZjBgxggULFpCcnIy3tzdxcXE0adKkUbrVhev1pBvDkiVLSEhIICkpyfrZxo0bWbRoEQkJCY0u/1q2mzp1Ku3bt6/xurra7vTp0yQmJpKfn19jOTt37mTz5s2YzWZuu+02/vznP9O9e3e2b98OWJzyyJEjG62no3C5Md3ExEQmTZrEoUOH+Oyzz6yfnzhxgpSUFPbv3+dzBnYAACAASURBVM+8efOqXRcbG0thYSFffPEF//73v4mMjGTq1KnVzmvevDmTJ08GLL3YhIQEevfuzZ49eygpKaFJkyZ8++23rFixghdffJGEhATOnj1LZGSk/ZR2A2qz26pVq9i6dSsFBQU88MAD1a57++23yc3N5bPPPiMgIICBAwfWaDeAjz76iLS0NLZs2cLNN99MSUkJDz74IIsWLSIgIIDmzZuzceNGduzYwbBhw1i9ejVZWVkUFxfbRMdPPvmEo0ePcvnyZcaPH09sbCxdunRhyJAh7Nu3j3Xr1nH16lUyMjJ45JFHSE5ORilFVFQU06dPJzw8nIMHD9KpUyd69+7Nrl276NWrF927d+fixYusWbPGWteQIUPo0qULYHkji4uLIyYmxjoMtn37duLi4oiNjUVEsMxHaRi12W7Dhg0cPny4Wo+1nLra7pZbbrE+c3l5eZhMJgYPHsymTZvo1q0bixYt4vjx46SkpHDw4EGSk5P5z3/+Q3x8PPn5+YgITZs2bbB+DkdEnOawiFM/3nzzTdm/f3+1z//+979LSkpKvcuzJ2X6GX6f7XXUx3612e0vf/mL7N69u87lOIrabFdR52eeeUY++OADiY6OlgMHDsiyZcvk2LFjsnz5cpkxY4aIiHzwwQeyd+9e+eijj+Tbb7+VXbt2yZo1a+T5558XEZFJkyZJQUGBzJs3z3qNiMj58+dl0aJF1uPgwYPW72JiYkRE5NVXX63xs9LS0jrpUpteVXEV2znr8+ZygbSKREdHEx8fbxdZagqivf/++5w5cwYvLy9efPFFYmJi8Pb2Jioqii1btli/mzFjRo1lOuvAvq1wJftNmDChUlDt2LFjXL58mYyMDJYuXVqtvLoE0j7++GOOHz+Ov78/jz32GCkpKQwcOJCUlBTy8/O56aab8PLyIigoiJtuuonExERrTzc6OpqQkBBOnjxJXFwcJpOJ7t2784c//KFeunbu3JmSkhLOnDljDSBW7V3WN5BWG0bab8aMGQwcOJCBAwfy7LPP0rx5c9577z0OHz5slclZnzenH15wple2vXv3smTJEiIiIhgwYABBQUHccccdrF+/nszMTOt3Io17nXMnnNV+EydO5Ny5c+Tl5dGhQwdOnjzJqVOnaNGiRYN1rTrEVJ45U1MGDcCiRYus/+/QoYP1vIyMDC5cuMDQoUPrVO+kSTXvBzlkyJA6XX8tnNV+IkLr1q25cOECSim+/vpr7r77bg4fdv4NW5w+kPbll1/Stm1bRITCwkIeeughxo0bx+7du7nrrrsYPXo0AOHh4Zw6dYqIiAgiIyMxm834+/sTGRnJhQsXGDlyJElJSezbt4/u3bsDlqGVmoJptREZGcnChQspLCykV69enD59mk2bNuHt7V3pO+1wf8FZ7ZeVlVUpqHb+/HkSExMpLCzEiLe/ij3G22+/ncWLF+PlZXyfyFntp5Ri7dq1hIeHk5ycTGpqKqmpqezatcuaTeGsGG/V6xAWFsbx48fp2LEjPj4+eHp6lr820KZNG1atWoWXlxdNmjQhODi40itbSkoKy5cvp1mzZvj4+FBaWmpNcwEICAioNShTUxBNRPD09CQyMtKaIlZUVMSzzz7LkSNHrN9pfsFZ7desWbNKQbW0tDRMJpOhbymNfV2vmKd86tQpZs6cyahRo2rMm60rzmq/rKwsPvjgA06cOMHUqVN58sknAcjMzKR58+YN1tcRuPSY7vWo2IjLcwQXLVpkWA/CWceYbIU7268uY7rLly/Hy8uLHj164OPjw9atW8nKyiIuLo7Q0FBCQkLIz8/Hw8ODRx99FLPZTNu2bTl27Bhz5sxh7ty59OrVy/o6/9xzz7Fy5Uq6d+9uTanLyMhgw4YN1vpHjx5NYGCg9e+K98xsNlNQUFDN6dpqTPd6GG0/Z33enH54oTFU7DW88847jXply83NZdCgQRw4cIBNmzYxf/58Jk6cCFh6GLWN22kajr3sl5ycTFxcHH/84x8pKSnBZDJhMplITU1tlLz33Xcf58+f58qVK+Tl5eHr62vN97733nuZNm0aXl5eREdH89133wGWiQc9e/YkPT0dqPw6HxAQgL+/P2fPnrXWUfWV3Jk6TVWpbciksTnLL7/8MnFxcZXGg10Jpx9eKKexvQioHBSoby9i5cqV1lkvYWFhhIWFERUVBWCNPmtqx5ns5+fnx8mTJ/Hx8eHy5cs0bdqUKVOmEBMTU2nqaX25cOECzZo1Y//+/Xh4eODv709hYSFgmUnl4eFh/bfcWa5bt44jR44wdOhQkpKSKr3O5+TkWMsrKSnB09OTjh071ilP+Z577uHjjz+muLiYPn368Ktf/arBelXFaFtevnyZoqIiQkJCbKaTI3EZp3vfffexefNmrly5QklJSY29iPnz5zN9+nTeeecdwNKL+Prrryv1IoKDg7l06dI1exEV/wY4efIkmZmZnDx5ktLSUrp163bNWVSa6jiT/QoLC1m6dClvvfUWRUVFNtOxprHT8rHG8l5fudOJjo5mzpw5vPTSS/j6+lY6pyITJkyoc/2jRo1i1KhR1r/L76OtMdKWeXl5PPDAAzz//PO8+uqr1aYeuwIu43SN7EW0b9+ehQsXsnr1aoKCgqrNovr444+tPYyHH37YoffFVXAm+/373/9mzpw5nD9/npYtW1aaBeVIyh2wq2GkLb28vEhLSyMzM7NSUM6lMHp2RsWDBsxIq43XXntN8vPzbVaeLcBJZ8jY6nBn+9Vmu+vpXHFWWUPYvHmzdXbZ3LlzZe7cubJt27Zq5y1dutQ6q+3kyZMyduxY+Z//+R8REdm6dWslOerSDt3Bls76vLlMT7e+uGovQmPBlewXGxvLvHnzeO2115gwYQLr168nPT2dxYsXA79kEfj6+lJQUMCePXvw8PDg5ptv5vnnnwcgNTWVnTt3AtCkSRNeeukla/n9+vWzrsBV29hzxbjCbbfdxrhx4ygoKLBeX3HVLkfjSrZ0BG6dvaDROILQ0FC++OILvL29KS0tpbS0FKWUdbnQ8rzWkpISwLImc2BgIJcuXbKWUVpaWikroSqWjtsvFBUVWcvTuBYu53QbmyVgNpuJjY2luLgYk8nEpEmT2Lp1K+np6QwbNswaEIiNjSUhIaHGpfHeeustYmJirHPtY2NjmT59OsXFxaxZswaTycTf//53zpw5w/DhwxslrzthL9vt3LmTuXPnMnv2bEpLS5k0aRLR0dHs2LGj0vVVr0tLSyMxMZGQkBByc3Mrpf7Vx3b9+/dn/vz5hIeHc/ToUUpKSirNsLrzzjtJSUmxrtAVHBxMTk6OdWYWWHqjU6dOZerUqdZUxIoEBARUGnteu3YtJ06csH5fMXMhJyeHjz/+mH/84x+cP3++fje5gdjKtgBLly4lOjq6xhS+kSNHkpiYyOnTpzl8+DDjxo1rVL1G4JTDC454XfPy8mLWrFns3r2bvXv3EhISUmkR9Ly8PH7++WeCgoKqyfenP/2JrKws3n33XU6fPs0rr7zC9u3b2blzJyEhIcybN49evXrRrl0761zyGwUjbHf06FFMJhPLli3jp59+onXr1owfP55Zs2ZVsl/V6/74xz/Ss2dPfvrpJ5o3b17pFb0+tlNKVXIQwcHB1v/37NkTsCzgUk59Zoi1atWKr776ioEDBzJr1izr5127dq00KeJamQvffPMNd911V53rrA1H2BYgJCSEhQsX4ufnV02GipsGdO3albZtXW/XIafs6TridQ0sa/AmJSUxduzYat/deuutJCYmkpaWRklJSaXUopycHN58802mTJmCSOVpo506deLdd9/lxx9/tMm9cDWMtp2XlxcBAQGsWbOGli1bVnsNr3rdhg0b6rywTE34+vpmKaWw19GjRw8mT55c7fM2bdrUuYyQkBCefvpp69++vr5ZDdHVUba95557WLJkCd9//301+7nDpgFO2dPt378/wcHBrFixotbXtQULFgAwaNCgWl/XrpXDV1RUREREBCNGjGD79u20b9++0iLox44dY968efz6179m8+bNeHt7W9PBhg8fTkhICGazmYiICGu+Z1xcHHPnzqW0tLSSLDcSRtjuiSeewGQyUVxcTLt27fD09KSgoICxY8eydu1a+vfvT6dOnapd17dvX7Zt22ZdyrHiK3pdU/9upA0eHWHbixcv8s4773Dp0iXrMEq5/a5eveoemwYYnT5R8cCGaSq18cMPP0hiYmK9rsnKympQXadPn5bZs2db/8ZJU1hsddjbftp27mPLqtRm22vZ79ChQ/LGG2/U+r2z2sypFrxxxe2s64O7b33tzvZzd9tVxR1s6aw2cyqna0uUUt7AfuA5Edlcz2sHACuA34iIbTbQ0tQLpdSjwBLgtyJSr9wopdRHwB4Rsc+2BprropT6O7BDROq1M6ZSqh2wD+ghIj/ZRTiDcWen+wwwWkRCG3j918CHIvK+bSXTXA9liUzuAN4SkY8bcH03YCvQVUQuXe98jW1RSt0L/BPoIiJ5Dbh+EdBUROq+8IQL4ZZOVynlAxwC/ktEvm1gGX2BvwJ3iMhVW8qnuTZKqd8DcUBPEbn2dgK1l7EGOCYir9tUOM11UUp9DvxLRKpvNle361sDB4D7RCTDpsI5AU6ZMmYDngH2NdThAohIKpbhiWdsJpXmuiilPIC5wOyGOtwy5gIvK6UCbCOZpi4opR4A7gHebWgZInIWeAeYdb1zXRG36+kqpfyAw8AwEdnVyLL6ABuwvKbm20I+zbVRSv0BeBXoI41snEqpd4FzIhJjE+E010Up9S/g7yLSYKdbVk5LLM9xkIgcsYlwToI7Ot1XgBARCbdRecmAWUQSbVGepnaUUp7A/wFTReR/bFBeB+B7oFtZ70ljR5RS/YDVWO53oxcqVkrNwjK891Rjy3Im3MrpKqWaA0eAR0Xk/2xU5t3Al1iCAj/bokxNzSilRgNRQN/G9nIrlLkUKBSRmrdb0NiEsuCnGXhfRGyyj45SqgWW5/l3IuI2UzzdzelGA/eKiE2nqyil1gP/FpGFtixX8wtKKS/gR+BFEfnKhuW2A9KxpCCdtlW5msoopR4B/gzcZcs0S6XUDKCXiIy0VZlG4zZOt8Kv4sMist/GZf8G2IKlt3vZlmVrLCil/giMBfrbqpdboewEwEdEXrZluRoLZb3cb4G3ReRvNi67GZbneqCI7LVl2UbhTk53Nhan+LSdyl8LHBIRkz3Kv5FRSjUBDgJPicg2O5RfnoJ0r4icuN75mvqhlHocWAjc08iMk9rKn4xliGHYdU92AdzC6ZalBR3CjpFOpVQXLAn7XUXkoj3quFFRSr2AJduk7mse1r+O+UArERlvrzpuRMp6ubuAOBFJslMdvlh6u+EistMedTgSd3G6cUCgiDxn53pWAZkiMtOe9dxIlD1Qh4E/iEiaHesp/2F+QESO2queGw2lVDiWfNpeth4WqlLPi8ATIjLIXnU4Cpd3uo6cvaKUuh3YjU5BshlKqUlAqIg84YC6XgM6i0j1BZQ19aZsIsteIFpE7LoJW9kQ1CHgybKJSy6LOzjdBMDPUfO0lVJ/AX4WkWmOqM+dUUo1BY4Cg0RkjwPquxlLrzpERA7Yuz53Ryk1CpgMPGjPXm6F+v4bGCMiA+xdlz1xaadbYUWi3zoqHUgpdSvwA5bUmDOOqNNdUUpNA+4XkREOrDMaS8DnvxxVpztSluKXDkSJyP86sM79wPMi8rUj6rQHru50lwJFIvInB9e7GPAUkeo7CGrqhFLqJizBkf6OTHyvMIEmTER+cFS97oZSaizw31iyChzmRJRSTwIvAcGOrNeWuKzTrTDF8zciku3guttgSeTvKSInHVm3u6CUmollbHyMAXW/AvQTkQhH1+0OlK1VfRAYJyJbHVx3+VTxKSKS4si6bYUrO90VwAURedWg+uMBfxF5wYj6XZkKi5k8KCKHDai/fFGkoSLyb0fX7+oopcYDw0XkUYPqHwFMxzI05XIOzCWdrlKqM7ATy2IY5w2S4VdYoql9ROSYETK4KkopE3CLiBi2bKZSagLwuIgMNkoGV6Qsxe8QMEJEvjNIBg8sWUSvicinRsjQGFzV6a4GjovIHIPleB3oICJ/NFIOV0Ip1QrLq2kvETluoBzlC92PEpHtRsnhaiilXsayoNTvDZbjCcCEZZahzWfB2ROXc7pKqTuBbVim/Bq6FYtSyh/La2pfETlkpCyuglLqDaC5iLzkBLI8i8XpPmK0LK5AWYrfEWCwiHxvsCwK+A5YJCKfGClLfXFFp/s34AcRmW+0LABKqVgs6WOjjZbF2VFKtcWS4ne3OMGmg+qXzUufFRGzweI4PUqpKcBDIvIHo2UBUEo9BizGsoJcvTYvNRKXcrpKqR7AJiy93Fyj5YFKqU+hIpJutDzOjFJqCSAiMtloWcpRSj0FjMcyYcJ1HgYH44ztvKy3+w2wXEQ+NFqeuuJqTjcJSBWRN42WpSJKqalYIvFO0QNwRpRS7YE9WN4KMo2Wp5yyFKR0YJKI/MtoeZwVpVQMlklITvVGp5T6HbAKS+poo3ercAQu4XSVUqFAKyyvEg3a1tmeVBjrmgyct+Ui3O5A2dJ83YBLIjLDaHmqopSKBKZg2UzxYxG5YrBITkNZDOVu4C9YJiQcNFikaiilvgI+xdK+bLJrhT1xld2A/wDMBL4CbjdYlpq4Hfgay2pLOuG+Om8CIwFn3e7IF7gZiAU6GyyLsxEMxGBJ0WpvsCzVKNu84Ecsm5kaHpytC15GC1BH2gHdgatAjsGy1EQO8BssMjo82d8F8MDS1pz1R74AaAs0BzwNlsXZuAnLlupnAGdcR7oAi39oCRQaLEudcNaHoCpXgX9iGTd1ukVmymR6EIuMLjGu5CjKgh1ngaeNzquuDRH5GAjCIqdL5Xw6AA8sPcm7nXH2nohcBUYACYChKaR1xSXGdDUajcZdcJWerkaj0bgF2ulqNBqNIxGRWg9fX99MQNz18PX1zXQnfd1Nn/ro6w66VrWfqz+XddXHlXSyhb7XHNNVSrn1JB2lFCKiKvzt0vq6mz7Xo6K+7qBrVftd4zyX0LWu+pSd6xI6XYu66mvTlLHo6Gji4+NtWaSVJUuWUFRUROfOnYmIsKTCvv/++5w5cwYvLy8mTJjAjBkzaNGiBaGhoZw4ccL63YwZtsnHN1q/VatWcejQIfr06cNvfvMbzGYz7du358knn7SpLEbrWdGOjzxiu7VojNarov3at29Peno6ycnJmM1mm8vjaF03btzI/v37UUoxdepUm9fpaH3effddLl++TEZGBkuXLmXevHnk5uYSHx9Peno6M2fOJD4+nm7dutW7vno73U8++YSjR49y+fJlxo8fT2xsLF26dGHIkCHs27ePdevWcfXqVTIyMnjkkUdITk5GKUVUVBTTp08nPDycgwcP0qlTJ3r37s2uXbvo1asX3bt35+LFi6xZ88uEkiFDhtClSxcAsrOziYuLIyYmxnpj9u7dy5IlS4iIiGDixImcO3eOvLw8OnTowOeff279TkSwZC65rn7Tp09n8uTJxMTEMGLECGJiYujQoQMN7R04q55V7eguelW1X/PmzenRowc//9zw+SLOpOv27duJi4sjNja2Xs+bs+rj7e3NqVOnaNGiBQAzZ84kOjoagN/+9rcMGzasARazUO9A2pdffknbtm0REQoLC3nooYcYN24cu3fv5q677mL0aMvU7PDwcE6dOkVERASRkZGYzWb8/f2JjIzkwoULjBw5kqSkJPbt20f37t0BEBGKi4utR2nptVMmIyMjWbhwIYWFhWRlZTFixAgWLFhAcnJype/q0wCcVT+lFAUFBRQXF9O8eXMyMzOZPHkyu3btapDjdVY9q9rRXfSqaj+AtWvXMmZMw3crciZdy2nMEIEz6XP+/HkSExMpLCxslE41Ue+eblhYGMePH6djx474+Pjg6elZPpZBmzZtWLVqFV5eXjRp0oTg4GASExOtv0YpKSksX76cZs2a4ePjQ2lpKX369LGWHRAQUOurSWBgIAkJCfTu3Zs9e/ZQUlKCiODp6UlkZCTNmjVj48aN7Nixg2HDhlX6zh30A0hKSiI8PByAMWPGMG/ePHx9fRvUq3BWPava0V30gsr2A8jIyOD222+vt47OqOuDDz7IokWLCAgIaFB7dDZ9PDw8MJlM1l77ihUrSEtLY8+ePQQEBJCSksL+/fuZN28e3t7e9VP0WlE2y9e2Y8aMGdb/Hz9+XCZPnixFRUU2raM+lOlnM32N1s/W+tSG0XqWU1FfW+hqtF5V7Vfb4Sq61lUfcYNnT6Tu+ursBTeK9rubPtdDZy84Nzp7oWYMmxxRPijdEIqLizGZTEyaNImtW7fy+eefM2fOHF599VVKS0uZNGkS0dHR7Nixw4YS14/G6AewbNkyXnjBstHwzp07WbBgAc888wxFRUW89dZbxMTE8PHHH9tC1AbRWP1yc3MZNGgQBw4cYNOmTcyfP5+JEyc6jf2g8TqOHDmSxMRETp8+TXp6OsOGDePAgQM2ks522NKWzoiz6dfglLHly5fj5eVFjx498PHxYevWrWRlZREXF0doaCghISHk5+fj4eHBo48+itlspm3bthw7dow5c+YAlaOVzz33HCtXrqR79+7W4EJGRgYbNmyw1jl69GgCAwPx8vJi1qxZ7N69m71795Kens4bb7zByy+/zE8//UTr1q0ZP348s2bNIigoyOX0A4iKirI2lj59+tCnTx9effVVCgoK+NOf/kRWVhbvvvtug3RzBv1WrlxpHd8MCwsjLCyMqKgoLly4YBP7OYOOgYGBnDt3Dg8Pj0ZHvJ1Zz4q21Ppdnwb3dO+77z7Onz/PlStXyMvLw9fX1/pLcO+99zJt2jS8vLyIjo7mu+8sOzUPGzaMnj17kp5u2e2jYrQyICAAf39/zp49a61DqkQcK75+nDhxgqSkJMaOHctzzz3HwoULOX36NH5+fgQEBLBmzRpatmzZUPUM168qH330EX379uWmm24iJyeHN998kylTprikfidPniQzMxOz2WzNUV22bBljxoyhVatWNrGf0TqW6zRlyhTee++9RunhzHrWZEut37VpcE/3woULNGvWjP379+Ph4YG/vz+FhZblLL28vPDw8LD+W67AunXrOHLkCEOHDiUpKalStDInJ8daXklJCZ6ennTs2LHGiGNRURERERGMGDGC7du3c/PNN+Pj40NoaCitWrXC09OTgoICxo4d21D1DNUPLE42LS2NLVu2UFBQwAcffEBYWBjBwcEMHz6ckJAQzGYzjz/+uMvp1759exYuXMjq1asJCgpi1apVbN26lYKCAh544AGb2M9oHa9evUpCQgJnz54lMjKSEydONC7i7aR6VrWlPXA7/a4VZcOG0e/XXntN8vPzbVaeLcCG0X5n0M+W+lTFGfSrCjbOXjBax6r2q+1orK6O0rOu+oibtNW66quzF9wo2u9u+lwPnb3g3OjshZppdPZCYyODZrOZ2NhYoHLE/vz588yZM4eEhARKSkqIjY0lISGBhISEamV8+OGHjBo1CoDTp0+zePFiJkyYQE5ODhs3bmTgwIGApVf/7LPPNjgKaUtdK0a2d+7cydy5c5k9e3a1a9LS0khMTCQkJITc3NxKWRuHDx9m3LhxjZLJXvY7cOAAiYmJPPbYYxw7doyZM2fy+uuvVwpWlFMxG8NsNhMVFcXq1asBy7z4hIQEkpKSOHPmDMOHD2+wrLbUdenSpURHR5OamnpN+1XMzIDK96ix+lwLW+pqMpkwmUykpqZWO69iZN8W7bGu2MuWVbGHveo0phsbG8u8efN47bXXmDBhAuvXryc9PZ3FixdbFSgoKMDX15eCggL27NmDh4cHN998M88//zyAtXECNGnShJde+mUPuX79+gGVI/Z/+9vf8PT0tE7Xy8vL4+eff65xXGXMmDHWAfNbbrmFLl268M033+Dt7c2QIUPYtm0bYPklCg4OdgpdK0a2P/30U0wmE8uWLePMmTO0a9fOev79999Pz549+emnn2jevHmlrI2QkBDatm17HesZY79u3brRrVs3jh07RufOnbly5QqLFy9m5MiR1SLBFbMxbr/9dpo2bcrly5eB6vPiy+fLG61rSEgICxcuxM/Pj6SkpFrtVzEzo+o9ateu3XX1MVrXixcv0rRpU6ZMmUJMTAx9+/atJEPFyH7Xrl3r1B6dST+obMuq2NJe5dSppxsaGsoXX3yBt7c3paWllJaWopTiyJEjgLVbTUlJCQDbtm0jMDCQS5d+2bKotLS0UnTwely9epX+/fvTsWNHduzYwa233kpiYiJpaWmUlJRQVFT7VmS///3vefrppzl58mRd1DNE12tFtvPz8yv9vWHDBoYOHQpUztpwNp2q8t1333H//fcDlh+PhIQE2rRpU81+FbMxgoKCeOONNygpKeGnn36qs46O1vWee+5hyZIlfP/999W+q2q/8swMW+MIXau+8hcVFVnLs3fmghG2rKifvahTT7d///4EBwezYsUKjh49SklJSaVFI+68804WLFgAwKBBgwgODiYnJ8e62ARYflXKf1lqo2LEPiIigsTEROtEiL/+9a/MmzePX//612zevBlvb28efvhhAL744gvS0tL4/PPPufXWW9m0aRMZGRn07duX1NRU0tLS+PDDD+vU8B2ha9XIdvk87+LiYtq1a8esWbMwmUzW87dt28bSpUurZW1U7XUYqRNUtt/DDz/MP/7xD15//XUAa2R51KhR1exXMRvjV7/6FZs3b+bUqVO0adOm0rx4Z9H14sWLvPPOO1y6dInBgwfTo0ePWu1XNTPj448/rnSPGoMjdA0ICCAvLw+TycTgwYNZu3Yt/fv3p1OnTnbPXDDClhX1g+pt2iZcK8qGnebuV+SHH36QxMTEel2TlZXVoLpKS0vl9ddfl1On7a3wYwAAAUxJREFUTomIfaP9NVEXXUtLS+Xs2bN1Ku/QoUPyxhtvWP92tD4ijrXf6dOnZfbs2da/sXH2wvWwtf2upc+1DiN1vZbtrtcer3U4QqeKNES/qvaqSl311dkLbhTtdzd9rofOXnBudPZCzVxzeMHX1zdLKdXGdmI5F76+vllV/3Zlfd1Nn+tRUV930LWq/a51nivoWld9ys91BZ2uRV31vWZPV6PRaDS2RW/BrtFoNA5EO12NRqNxINrpajQajQPRTlej0WgciHa6Go1G40C009VoNBoHop2uRqPROBDtdDUajcaBaKer0Wg0DkQ7XY1Go3Eg2ulqNBqNA9FOV6PRaByIdroajUbjQLTT1Wg0Ggeina5Go9E4EO10NRqNxoFop6vRaDQORDtdjUajcSDa6Wo0Go0D0U5Xo9FoHIh2uhqNRuNAtNPVaDQaB/L/AYSbictNV8O5AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "clf = DecisionTreeClassifier(criterion = \"entropy\", max_depth=3)\n",
    "clf = clf.fit(training_pca, training_label)\n",
    "\n",
    "tree.plot_tree(clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q6. Now that the tree has been learned from the training data, we can run the test data through and predict classes for our test data. Use the `predict` method of `DecisionTreeClassifier` to classify the test data. Then use `sklearn.metrics.accuracy_score` to print out the accuracy of the classifier on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.47619047619047616"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "test_pred = clf.predict(test_pca)\n",
    "\n",
    "accuracy_score(test_label, test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q7. Note that the DecisionTree classifier has many parameters that can be set. Try tweaking parameters like split criterion, max_depth, min_impurity_decrease, min_samples_leaf, min_samples_split, etc. to see how they affect accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5238095238095238"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_clf = DecisionTreeClassifier(criterion=\"gini\", max_depth=4, min_impurity_decrease=1, min_samples_leaf=2, min_samples_split=3)\n",
    "new_clf = new_clf.fit(training_pca, training_label)\n",
    "new_pred = new_clf.predict(test_pca)\n",
    "accuracy_score(test_label, new_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Using K-fold Cross Validation\n",
    "\n",
    "You have now built a decision tree and tested it's accuracy using the \"holdout\" method. But as discussed in class, this is not sufficient for estimating generalization accuracy. Instead, we should use Cross Validation to get a better estimate of accuracy. You will do that next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q8. Use `sklearn.model_selection.cross_val_score` to perform 10-fold cross validation on your decision tree. Report the accuracy of the model. For this step, revert back to using the original data before it got scaled and underwent PCA. So you want to use the data (features and labels) that came out of Q1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1151, 19)\n",
      "(1151,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.632518740629685"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "print(features.shape)\n",
    "print(labelcol.shape)\n",
    "\n",
    "clf = tree.DecisionTreeClassifier(criterion = \"entropy\")\n",
    "\n",
    "scores = cross_val_score(clf, features, labelcol, cv=10)\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q9. Now we want to tune our model to use the best parameters to avoid overfitting to our training data. Grid search is an approach to parameter tuning that will methodically build and evaluate a model for each combination of algorithm parameters specified in a grid. \n",
    "* Use `sklearn.model_selection.GridSearchCV` to find the best `max_depth`, `max_features`, and `min_samples_leaf` for your tree. Use 'accuracy' for the scoring criteria.\n",
    "* Try the values [5,10,15,20] for `max_depth` and `min_samples_leaf`. Try [5,10,15] for `max_features`. \n",
    "* Use a 5-fold cross validation. \n",
    "* Print out the best value for each of the tested parameters.\n",
    "* Print out the accuracy of the model with these best values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 20, 'max_features': 15, 'min_samples_leaf': 20}\n",
      "Accuracy: 66.20330147697653\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "params = {\"max_depth\": [5,10,15,20], \"max_features\": [5,10,15], \"min_samples_leaf\": [5,10,15,20]}\n",
    "\n",
    "grid_search = GridSearchCV(clf, params, cv=5, scoring=\"accuracy\")\n",
    "\n",
    "grid_search.fit(features, labelcol)\n",
    "\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "print(\"Accuracy:\", grid_search.best_score_*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q10. To perform the nested cross-validation that we discussed in class, you'll now need to pass the `GridSearchCV` into a `cross_val_score`. \n",
    "\n",
    "What this does is: the `cross_val_score` splits the data in to train and test sets for the first fold, and it passes the train set into `GridSearchCV`. `GridSearchCV` then splits that set into train and validation sets for k number of folds (the inner CV loop). The hyper-parameters for which the average score over all inner iterations is best, is reported as the `best_params_`, `best_score_`, and `best_estimator_`(best decision tree). This best decision tree is then evaluated with the test set from the `cross_val_score` (the outer CV loop). And this whole thing is repeated for the remaining k folds of the `cross_val_score` (the outer CV loop). \n",
    "\n",
    "That is a lot of explanation for a very complex (but IMPORTANT) process, which can all be performed with a single line of code!\n",
    "\n",
    "Be patient for this one to run. The nested cross-validation loop can take some time. An asterisk [\\*] next to the cell indicates that it is still running.\n",
    "\n",
    "Print the accuracy of your tuned, cross-validated model. This is the official accuracy that you would report for your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6255247376311843"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(grid_search, features, labelcol, cv=10).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q11. Our accuracy rate isn't very good. We wouldn't want to use this model in the real world to actually diagnose patients because it is wrong about 40% of the time! What could you do to improve the accuracy of the model? Answer as a comment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nMore dimensionality reduction would improve accuracy. If two or more features are close/related too much, eliminate one\\nof the features. Possibly collect more data on the matter so that the train percentage can be more meaningful. Have a set\\nmax depth for our trees so that errors are reduced.\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "More dimensionality reduction would improve accuracy. If two or more features are close/related too much, eliminate one\n",
    "of the features. Possibly collect more data on the matter so that the train percentage can be more meaningful. Have a set\n",
    "max depth for our trees so that errors are reduced.\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
